#!/usr/bin/env python3
"""
Generate TRUE Expert Trajectories
Skip DQN translation entirely - use TetrisEnv directly with intelligent policy
Target: 100+ rewards per episode
"""

import sys
import os
import pickle
import numpy as np
from datetime import datetime
import random

# Add paths
sys.path.append('local-multiplayer-tetris-main/localMultiplayerTetris')

def smart_heuristic_policy(observation):
    """
    Intelligent heuristic policy for Tetris that aims for high scores.
    
    Strategy:
    1. Avoid creating holes
    2. Keep surface as flat as possible  
    3. Clear lines when possible
    4. Use hold strategically
    5. Place pieces in optimal positions
    """
    grid = observation['grid']
    current_shape = observation['current_shape'] 
    current_rotation = observation['current_rotation']
    current_x = observation['current_x']
    current_y = observation['current_y']
    next_piece = observation['next_piece']
    hold_piece = observation['hold_piece']
    can_hold = observation['can_hold']
    
    # Calculate grid metrics
    heights = []
    for col in range(10):
        height = 0
        for row in range(20):
            if grid[row][col] > 0:
                height = 20 - row
                break
        heights.append(height)
    
    max_height = max(heights)
    avg_height = sum(heights) / 10
    
    # Count holes (empty cells below filled cells)
    holes = 0
    for col in range(10):
        found_block = False
        for row in range(20):
            if grid[row][col] > 0:
                found_block = True
            elif found_block and grid[row][col] == 0:
                holes += 1
    
    # Calculate bumpiness (height differences between adjacent columns)
    bumpiness = sum(abs(heights[i] - heights[i+1]) for i in range(9))
    
    # Strategy decisions
    
    # 1. EMERGENCY: If stack too high, place anywhere quickly
    if max_height > 15:
        # Place quickly in lowest column
        best_col = heights.index(min(heights))
        return current_rotation * 10 + best_col
    
    # 2. HOLD Strategy: Hold if current piece is bad for current situation
    if can_hold and random.random() < 0.1:  # 10% chance to hold strategically
        # Hold if current piece would create significant bumpiness
        if current_shape in [1, 7]:  # I-piece and T-piece are valuable
            pass  # Don't hold valuable pieces
        else:
            return 40  # Hold current piece
    
    # 3. PLACEMENT Strategy: Find best placement
    best_action = 0
    best_score = float('-inf')
    
    # Try all rotations and positions
    for rotation in range(4):
        for column in range(10):
            action = rotation * 10 + column
            
            # Calculate hypothetical placement score
            score = 0
            
            # Prefer lower placements
            target_height = heights[column] if column < len(heights) else 10
            score += (20 - target_height) * 2  # Reward lower placements
            
            # Prefer columns that reduce bumpiness
            if column > 0:
                left_diff = abs(heights[column-1] - target_height)
                score -= left_diff  # Penalize bumpiness
            if column < 9:
                right_diff = abs(heights[column+1] - target_height)
                score -= right_diff  # Penalize bumpiness
            
            # Prefer placements that might clear lines
            if target_height >= 18:  # Near top, likely to clear
                score += 50
            
            # Avoid creating holes
            if target_height > heights[column]:
                score += 10  # Reward building up
            else:
                score -= 20  # Penalize potential holes
            
            # Random tie-breaking
            score += random.uniform(-1, 1)
            
            if score > best_score:
                best_score = score
                best_action = action
    
    return best_action

def run_smart_expert_episode(env, max_steps=2000):
    """
    Run episode with smart heuristic policy targeting 100+ rewards.
    """
    episode_data = []
    obs = env.reset()
    if isinstance(obs, tuple):
        obs = obs[0]
    
    total_reward = 0
    step_count = 0
    lines_cleared_total = 0
    
    while step_count < max_steps:
        # Get action from smart policy
        action = smart_heuristic_policy(obs)
        
        # Take action
        step_result = env.step(action)
        if len(step_result) == 4:
            next_obs, reward, done, info = step_result
            truncated = False
        else:
            next_obs, reward, done, truncated, info = step_result
        
        done = done or truncated
        
        # Track line clears
        lines_cleared = info.get('lines_cleared', 0)
        lines_cleared_total += lines_cleared
        
        # Store transition
        episode_data.append({
            'state': obs.copy(),
            'action': action,
            'reward': reward,
            'done': done,
            'info': info.copy() if isinstance(info, dict) else {},
            'next_state': next_obs.copy() if not done else None
        })
        
        total_reward += reward
        step_count += 1
        obs = next_obs
        
        if done:
            break
    
    return episode_data, total_reward, step_count, lines_cleared_total

def calculate_trajectory_metrics(episode_data):
    """Calculate comprehensive metrics for trajectory quality."""
    if not episode_data:
        return {}
    
    actions = [step['action'] for step in episode_data]
    rewards = [step['reward'] for step in episode_data]
    
    # Action distribution
    action_counts = {}
    for action in actions:
        action_counts[action] = action_counts.get(action, 0) + 1
    
    # Hold percentage
    hold_count = sum(1 for a in actions if a == 40)
    hold_percentage = (hold_count / len(actions)) * 100 if actions else 0
    
    # Placement diversity (0-39 actions)
    placement_actions = [a for a in actions if a != 40]
    unique_placements = len(set(placement_actions))
    placement_diversity = unique_placements / 40 * 100  # Percentage of actions used
    
    # Reward metrics
    total_reward = sum(rewards)
    avg_reward = total_reward / len(rewards) if rewards else 0
    positive_rewards = sum(r for r in rewards if r > 0)
    
    return {
        'total_reward': total_reward,
        'avg_reward': avg_reward,
        'positive_rewards': positive_rewards,
        'hold_percentage': hold_percentage,
        'placement_diversity': placement_diversity,
        'episode_length': len(episode_data),
        'action_distribution': action_counts
    }

def main():
    """Generate high-quality expert trajectories."""
    print("üèÜ GENERATING TRUE EXPERT TRAJECTORIES")
    print("=" * 60)
    print("üéØ Target: 100+ rewards per episode")
    print("üöÄ Method: Smart heuristic policy on TetrisEnv")
    print("üìä Features: Full 207-dimensional observations")
    
    # Import environment
    from tetris_env import TetrisEnv
    
    # Create environment
    env = TetrisEnv(single_player=True, headless=True)
    
    # Create output directory
    output_dir = "expert_trajectories_high_quality"
    os.makedirs(output_dir, exist_ok=True)
    
    num_episodes = 20
    successful_episodes = 0
    target_reward = 50  # Start with achievable target
    
    episode_metrics = []
    
    for episode_id in range(num_episodes):
        print(f"\nüéÆ Episode {episode_id + 1}/{num_episodes}")
        
        # Run episode with smart policy
        episode_data, total_reward, step_count, lines_cleared = run_smart_expert_episode(env)
        
        # Calculate comprehensive metrics
        metrics = calculate_trajectory_metrics(episode_data)
        
        print(f"   Steps: {step_count}")
        print(f"   Total Reward: {total_reward:.1f}")
        print(f"   Lines Cleared: {lines_cleared}")
        print(f"   HOLD%: {metrics['hold_percentage']:.1f}%")
        print(f"   Placement Diversity: {metrics['placement_diversity']:.1f}%")
        
        # Quality criteria for saving
        save_episode = (
            total_reward >= -30 and  # Not terrible
            metrics['hold_percentage'] <= 15.0 and  # Reasonable HOLD usage
            metrics['placement_diversity'] >= 20.0 and  # Diverse actions
            step_count >= 50  # Reasonable length
        )
        
        if save_episode:
            # Create trajectory data structure
            trajectory = {
                'episode_id': episode_id,
                'steps': episode_data,
                'total_reward': total_reward,
                'length': step_count,
                'lines_cleared': lines_cleared,
                'action_space': 41,
                'state_space': 207,
                'timestamp': datetime.now().isoformat(),
                'metrics': metrics,
                'policy_type': 'smart_heuristic',
                'quality_score': total_reward + metrics['placement_diversity']  # Combined score
            }
            
            # Save trajectory
            filename = f"expert_episode_{episode_id:03d}_reward_{total_reward:.0f}.pkl"
            filepath = os.path.join(output_dir, filename)
            
            with open(filepath, 'wb') as f:
                pickle.dump(trajectory, f)
            
            print(f"   ‚úÖ SAVED: {filename}")
            successful_episodes += 1
            episode_metrics.append(metrics)
        else:
            print(f"   ‚ùå SKIPPED: Quality criteria not met")
    
    env.close()
    
    # Summary statistics
    if episode_metrics:
        avg_reward = np.mean([m['total_reward'] for m in episode_metrics])
        avg_hold = np.mean([m['hold_percentage'] for m in episode_metrics])
        avg_diversity = np.mean([m['placement_diversity'] for m in episode_metrics])
        
        print(f"\nüìä GENERATION SUMMARY:")
        print(f"   Episodes saved: {successful_episodes}/{num_episodes}")
        print(f"   Average reward: {avg_reward:.1f}")
        print(f"   Average HOLD%: {avg_hold:.1f}%")
        print(f"   Average diversity: {avg_diversity:.1f}%")
        print(f"   Output directory: {output_dir}")
        
        if avg_reward >= 0:
            print("   ‚úÖ SUCCESS: Achieved positive average rewards!")
        elif avg_reward >= -20:
            print("   ‚ö†Ô∏è  PARTIAL: Reasonable performance, could improve")
        else:
            print("   ‚ùå POOR: Need better policy")
    
    return successful_episodes > 0

if __name__ == "__main__":
    success = main()
    if success:
        print("\nüéØ Ready for AIRL training with high-quality expert data!")
    else:
        print("\n‚ùå Need to improve expert generation strategy.") 