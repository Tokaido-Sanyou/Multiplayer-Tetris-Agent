TETRIS REINFORCEMENT LEARNING SYSTEM
====================================

1. ENVIRONMENT (tetris_env.py)
-----------------------------
1.1 State Space
    - Grid: 20x10 matrix (0=empty, 1-7=piece colors)
    - Current piece: 4x4 matrix (0=empty, 1=filled)
    - Next piece: 4x4 matrix (0=empty, 1=filled)
    - Hold piece: 4x4 matrix (0=empty, 1=filled)
    Total state dimension: 248 (200 + 3*16)

1.2 Action Space (7 discrete actions)
    - 0: Move Left
    - 1: Move Right
    - 2: Move Down
    - 3: Rotate Clockwise
    - 4: Rotate Counter-clockwise
    - 5: Hard Drop
    - 6: Hold Piece

1.3 Reward Structure
    - Line Clears:
        * 1 line: 100 * level
        * 2 lines: 300 * level
        * 3 lines: 500 * level
        * 4 lines: 800 * level (Tetris)
    - Penalties:
        * Game Over: -1000
        * Per Step: -1
        * Stack Height: -50 * (height/20)
        * Holes: -100 * (holes/200)

2. ACTOR-CRITIC AGENT (actor_critic.py)
---------------------------------------
2.1 Network Architecture
    - Feature Extractor:
        * Input: 248-dimensional state
        * Hidden layers: [256, 128]
        * Output: 64-dimensional features
    
    - Actor Network:
        * Input: 64-dimensional features
        * Hidden layer: 64 units
        * Output: 7 action probabilities
    
    - Critic Network:
        * Input: 64-dimensional features
        * Hidden layer: 64 units
        * Output: Single value (state value)

2.2 Training Process
    - Experience Collection:
        * Store (state, action, reward, next_state, done) in replay buffer
        * Use epsilon-greedy exploration (ε decays from 1.0 to 0.01)
    
    - Actor Update:
        * Compute policy gradient using advantage estimates
        * Advantage = reward + γV(next_state) - V(state)
        * Loss = -log(π(a|s)) * advantage
    
    - Critic Update:
        * Compute TD error
        * Loss = (reward + γV(next_state) - V(state))²

3. REPLAY BUFFER (replay_buffer.py)
----------------------------------
3.1 Structure
    - Prioritized Experience Replay
    - Capacity: Configurable (default: 100,000)
    - Alpha: 0.6 (priority exponent)
    - Beta: 0.4 (importance sampling exponent)

3.2 Operations
    - Push: Add new transition with max priority
    - Sample: Sample batch using priority-based sampling
    - Update: Update priorities based on TD errors

4. TRAINING LOOP (single_player_train.py)
----------------------------------------
4.1 Main Loop Structure
    For each episode:
        1. Reset environment
        2. While not done:
            a. Select action using epsilon-greedy policy
            b. Execute action, get (next_state, reward, done)
            c. Store transition in replay buffer
            d. Sample batch and update networks
            e. Update exploration rate
        3. Log metrics and save checkpoints

4.2 Metrics Tracking
    - Episode rewards
    - Episode lengths
    - Lines cleared
    - Scores
    - Maximum levels reached
    - Actor and critic losses

5. EVALUATION
------------
5.1 Process
    - Run agent for fixed number of episodes
    - No exploration (ε = 0)
    - Track average reward and other metrics

5.2 Metrics
    - Average reward
    - Lines cleared per episode
    - Maximum level reached
    - Survival time

6. SINGLE PLAYER MODE
--------------------
6.1 Modifications
    - Disable player 2's:
        * Current piece
        * Next pieces
        * Hold piece
        * Locked positions
        * Score and level
        * Block pool

6.2 Benefits
    - Simplified state space
    - Focused learning on single player
    - Reduced computational complexity

7. MEMORY MANAGEMENT
-------------------
7.1 Optimizations
    - Reuse game surface
    - Clean up resources in close()
    - Store only necessary info in replay buffer

7.2 Error Handling
    - Try-except blocks for critical operations
    - Logging of errors and metrics
    - Graceful cleanup on failure

8. FILE STRUCTURE
----------------
8.1 Core Files
    - tetris_env.py: Environment implementation
    - actor_critic.py: Agent implementation
    - replay_buffer.py: Experience replay
    - single_player_train.py: Training script

8.2 Support Files
    - game.py: Game mechanics
    - utils.py: Helper functions
    - constants.py: Game constants

9. DEPENDENCIES
--------------
9.1 Core Libraries
    - PyTorch: Deep learning
    - NumPy: Numerical operations
    - Pygame: Game rendering
    - Gym: RL environment interface

9.2 Utility Libraries
    - Logging: Training metrics
    - OS: File operations
    - Random: Exploration 