#!/usr/bin/env python3
"""
Test script for enhanced DQN agent

This script verifies that the enhanced DQN implementation works correctly with:
- TensorBoard logging
- Checkpointing
- Batch operations
- Parallel environment compatibility
"""

import os
import sys
import torch
import numpy as np

# Add parent directory to path for imports
current_dir = os.path.dirname(os.path.abspath(__file__))
parent_dir = os.path.dirname(current_dir)
sys.path.insert(0, parent_dir)

from tetris_env import TetrisEnv
from rl_utils.dqn_new import DQNAgent

def test_single_agent():
    """Test basic DQN agent functionality"""
    print("Testing single DQN agent...")
    
    # Create agent
    agent = DQNAgent(
        learning_rate=1e-4,
        batch_size=32,
        buffer_size=1000,
        save_interval=10  # Save every 10 episodes for testing
    )
    
    # Create environment
    env = TetrisEnv(single_player=True, headless=True)
    
    # Test a few episodes
    for episode in range(5):
        obs_dict, _ = env.reset()
        
        # Convert observation to state
        grid_flat = obs_dict['grid'].flatten().astype(np.float32)
        metadata = np.array([
            obs_dict['next_piece'],
            obs_dict['hold_piece'],
            obs_dict['current_shape'],
            obs_dict['current_rotation'],
            obs_dict['current_x'],
            obs_dict['current_y'],
            obs_dict['can_hold']
        ]).astype(np.float32)
        state = np.concatenate([grid_flat, metadata])
        
        episode_reward = 0
        episode_length = 0
        done = False
        
        while not done and episode_length < 100:  # Limit steps for testing
            # Test action selection
            action = agent.select_action(state)
            assert 0 <= action <= 40, f"Invalid action: {action}"
            
            # Take step
            next_obs_dict, reward, terminated, truncated, info = env.step(action)
            done = terminated or truncated
            
            # Convert next observation
            grid_flat = next_obs_dict['grid'].flatten().astype(np.float32)
            metadata = np.array([
                next_obs_dict['next_piece'],
                next_obs_dict['hold_piece'],
                next_obs_dict['current_shape'],
                next_obs_dict['current_rotation'],
                next_obs_dict['current_x'],
                next_obs_dict['current_y'],
                next_obs_dict['can_hold']
            ]).astype(np.float32)
            next_state = np.concatenate([grid_flat, metadata])
            
            # Store experience
            agent.memory.push(state, action, reward, next_state, done)
            
            # Test training
            if len(agent.memory) >= agent.batch_size:
                loss = agent.train_step()
                if loss is not None:
                    assert isinstance(loss, float), f"Loss should be float, got {type(loss)}"
            
            state = next_state
            episode_reward += reward
            episode_length += 1
        
        # Test episode logging
        episode_info = {
            'score': info.get('score', 0),
            'lines_cleared': info.get('lines_cleared', 0),
            'level': info.get('level', 1)
        }
        agent.log_episode(episode_reward, episode_length, episode_info)
        
        print(f"Episode {episode + 1}: Reward: {episode_reward:.2f}, Length: {episode_length}")
    
    env.close()
    agent.close()
    print("Single agent test passed!")

def test_batch_operations():
    """Test batch action selection and training"""
    print("Testing batch operations...")
    
    agent = DQNAgent(batch_size=16, buffer_size=1000)
    
    # Create batch of random states
    batch_size = 8
    states = np.random.random((batch_size, 207)).astype(np.float32)
    
    # Test batch action selection
    actions = agent.select_actions_batch(states)
    assert len(actions) == batch_size, f"Expected {batch_size} actions, got {len(actions)}"
    assert all(0 <= action <= 40 for action in actions), "Invalid actions in batch"
    
    # Fill buffer with some experiences
    for i in range(100):
        state = np.random.random(207).astype(np.float32)
        action = np.random.randint(0, 41)
        reward = np.random.random()
        next_state = np.random.random(207).astype(np.float32)
        done = np.random.random() < 0.1
        agent.memory.push(state, action, reward, next_state, done)
    
    # Test batch training
    loss = agent.train_batch_update(num_updates=5)
    assert loss is None or isinstance(loss, float), f"Batch loss should be float or None, got {type(loss)}"
    
    agent.close()
    print("Batch operations test passed!")

def test_save_load():
    """Test save and load functionality"""
    print("Testing save/load functionality...")
    
    # Create and train agent briefly
    agent1 = DQNAgent(batch_size=16, buffer_size=1000)
    
    # Add some experiences
    for i in range(50):
        state = np.random.random(207).astype(np.float32)
        action = np.random.randint(0, 41)
        reward = np.random.random()
        next_state = np.random.random(207).astype(np.float32)
        done = np.random.random() < 0.1
        agent1.memory.push(state, action, reward, next_state, done)
    
    # Train a bit
    for _ in range(10):
        agent1.train_step()
    
    # Save state
    save_path = "test_checkpoint.pt"
    agent1.save(save_path)
    
    # Record state
    original_epsilon = agent1.epsilon
    original_steps = agent1.steps_done
    
    # Create new agent and load
    agent2 = DQNAgent(batch_size=16, buffer_size=1000)
    agent2.load(save_path)
    
    # Verify state
    assert abs(agent2.epsilon - original_epsilon) < 1e-6, "Epsilon not restored correctly"
    assert agent2.steps_done == original_steps, "Steps not restored correctly"
    
    # Clean up
    agent1.close()
    agent2.close()
    if os.path.exists(save_path):
        os.remove(save_path)
    
    print("Save/load test passed!")

def test_tensorboard_logging():
    """Test TensorBoard logging functionality"""
    print("Testing TensorBoard logging...")
    
    agent = DQNAgent(
        batch_size=16,
        buffer_size=1000,
        log_dir="test_logs"
    )
    
    # Verify TensorBoard writer is created
    assert hasattr(agent, 'writer'), "TensorBoard writer not created"
    assert agent.writer is not None, "TensorBoard writer is None"
    
    # Test logging
    agent.log_episode(100.0, 50, {'score': 1000, 'lines_cleared': 5, 'level': 2})
    
    # Clean up
    agent.close()
    
    # Clean up logs directory
    import shutil
    if os.path.exists("test_logs"):
        shutil.rmtree("test_logs")
    
    print("TensorBoard logging test passed!")

def main():
    """Run all tests"""
    print("Running DQN enhancement tests...\n")
    
    try:
        test_single_agent()
        print()
        
        test_batch_operations()
        print()
        
        test_save_load()
        print()
        
        test_tensorboard_logging()
        print()
        
        print("All tests passed! âœ…")
        print("\nEnhanced DQN agent is ready for training with:")
        print("- TensorBoard logging with comprehensive metrics")
        print("- Automatic checkpointing every 1000 episodes")
        print("- Batch operations for efficient training")
        print("- Parallel environment compatibility")
        
    except Exception as e:
        print(f"Test failed: {e}")
        import traceback
        traceback.print_exc()
        return 1
    
    return 0

if __name__ == "__main__":
    exit(main()) 