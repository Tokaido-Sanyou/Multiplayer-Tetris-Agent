#!/usr/bin/env python3
"""
COMPREHENSIVE DEBUG: Full 3 Batch 3 Phase System Test
Tests the complete pipeline:
1. Iterative exploration (ALL terminal states)
2. Enhanced 6-phase state model training
3. Piece-by-piece progression
4. Goal vector encoding/decoding
5. Board state inheritance
"""

import sys
import os
sys.path.append('localMultiplayerTetris')

import numpy as np
import torch
from datetime import datetime

from localMultiplayerTetris.rl_utils.enhanced_6phase_state_model import (
    PieceByPieceExplorationManager, 
    Enhanced6PhaseComponents
)
from localMultiplayerTetris.tetris_env import TetrisEnv

def debug_iterative_exploration():
    """Debug the iterative exploration to verify ALL terminal states are explored"""
    print('üîç DEBUG 1: Iterative Exploration (ALL Terminal States)')
    print('='*70)
    
    env = TetrisEnv()
    exploration_manager = PieceByPieceExplorationManager(env)
    
    # Small test for verification
    exploration_manager.max_pieces = 1
    exploration_manager.boards_to_keep = 3
    
    print('üöÄ Running iterative exploration with 1 piece...')
    exploration_data = exploration_manager.collect_piece_by_piece_exploration_data('iterative')
    
    # Analyze results
    total_possible = 4 * 10 * 10  # 4 rotations * 10 x positions * 10 y positions (filtered)
    actual_explored = len(exploration_data)
    unique_placements = len(set(d['placement'] for d in exploration_data))
    
    print(f'\nüìä EXPLORATION ANALYSIS:')
    print(f'   ‚Ä¢ Expected possible states: ~{total_possible} (before plausibility filter)')
    print(f'   ‚Ä¢ Actually explored: {actual_explored}')
    print(f'   ‚Ä¢ Unique placements: {unique_placements}')
    print(f'   ‚Ä¢ Exploration completeness: {unique_placements == actual_explored}')
    
    # Check coverage
    rotations = set(d['placement'][0] for d in exploration_data)
    x_positions = set(d['placement'][1] for d in exploration_data)
    y_positions = set(d['placement'][2] for d in exploration_data)
    
    print(f'   ‚Ä¢ Rotations covered: {sorted(rotations)} (all 4: {len(rotations) == 4})')
    print(f'   ‚Ä¢ X positions covered: {len(x_positions)}/10')
    print(f'   ‚Ä¢ Y positions covered: {len(y_positions)}/10')
    
    # Check reward distribution
    rewards = [d['terminal_reward'] for d in exploration_data]
    print(f'   ‚Ä¢ Reward range: {min(rewards):.1f} to {max(rewards):.1f}')
    print(f'   ‚Ä¢ Reward variety: {len(set(rewards))} unique rewards')
    
    if actual_explored > 100:  # Should be exploring many more states
        print('   ‚úÖ ITERATIVE EXPLORATION: WORKING (exploring many states)')
        return True, exploration_data
    else:
        print('   ‚ùå ITERATIVE EXPLORATION: FAILED (too few states)')
        return False, exploration_data

def debug_goal_vector_system():
    """Debug the 8D goal vector system"""
    print('\nüéØ DEBUG 2: Goal Vector System (8D Binary + Discrete)')
    print('='*70)
    
    components = Enhanced6PhaseComponents(device='cpu')
    
    # Test comprehensive goal vector scenarios
    test_cases = [
        {'placement': (0, 0, 10), 'confidence': 1.0, 'quality': 0.8, 'lines_potential': 4, 'is_valid': True},
        {'placement': (1, 5, 15), 'confidence': 0.5, 'quality': -0.3, 'lines_potential': 1, 'is_valid': True},
        {'placement': (2, 9, 19), 'confidence': 0.2, 'quality': 0.0, 'lines_potential': 0, 'is_valid': False},
        {'placement': (3, 4, 12), 'confidence': 0.9, 'quality': 1.0, 'lines_potential': 3, 'is_valid': True},
    ]
    
    print(f'üß™ Testing {len(test_cases)} goal vector encodings...')
    
    all_passed = True
    for i, test_case in enumerate(test_cases):
        # Encode
        goal_vector = components.goal_selector._option_to_goal_vector(test_case, 'cpu')
        
        # Decode
        decoded = components.goal_selector.decode_goal_vector_to_placement(goal_vector)
        
        # Verify
        original = test_case['placement']
        decoded_placement = decoded['placement']
        
        matches = (original[0] == decoded_placement[0] and 
                  original[1] == decoded_placement[1] and 
                  original[2] == decoded_placement[2])
        
        print(f'   Test {i+1}: {original} ‚Üí {decoded_placement} {"‚úÖ" if matches else "‚ùå"}')
        
        if not matches:
            all_passed = False
    
    if all_passed:
        print('   ‚úÖ GOAL VECTOR SYSTEM: WORKING')
        return True
    else:
        print('   ‚ùå GOAL VECTOR SYSTEM: FAILED')
        return False

def debug_6phase_training():
    """Debug the 6-phase training system"""
    print('\nüß† DEBUG 3: 6-Phase Training System')
    print('='*70)
    
    env = TetrisEnv()
    components = Enhanced6PhaseComponents(device='cpu')
    components.set_optimizers(state_lr=0.001, q_lr=0.001)
    
    # Generate some exploration data
    exploration_manager = PieceByPieceExplorationManager(env)
    exploration_manager.max_pieces = 1
    exploration_manager.boards_to_keep = 2
    
    print('üîÑ Generating training data...')
    exploration_data = exploration_manager.collect_piece_by_piece_exploration_data('iterative')
    
    if len(exploration_data) < 10:
        print('   ‚ùå Insufficient training data')
        return False
    
    print(f'   ‚Ä¢ Training data: {len(exploration_data)} samples')
    
    # Test state model training
    print('üéØ Testing state model training...')
    state_results = components.train_enhanced_state_model(exploration_data)
    print(f'   ‚Ä¢ State model loss: {state_results["loss"]:.2f}')
    print(f'   ‚Ä¢ Top performers used: {state_results["top_performers_used"]}')
    
    # Test Q-learning training
    print('üîÆ Testing Q-learning training...')
    q_results = components.train_simplified_q_learning(exploration_data)
    print(f'   ‚Ä¢ Q-learning loss: {q_results["q_loss"]:.2f}')
    print(f'   ‚Ä¢ Trajectories trained: {q_results["trajectories_trained"]}')
    
    # Test goal generation
    print('üéØ Testing goal generation...')
    test_state = torch.FloatTensor(exploration_data[0]['state']).unsqueeze(0)
    goal = components.get_goal_for_actor(test_state)
    print(f'   ‚Ä¢ Goal shape: {goal.shape}')
    print(f'   ‚Ä¢ Goal vector: {goal.squeeze().tolist()[:4]}... (first 4 elements)')
    
    if (state_results['loss'] < float('inf') and 
        q_results['q_loss'] < float('inf') and 
        goal.shape[1] == 8):
        print('   ‚úÖ 6-PHASE TRAINING: WORKING')
        return True
    else:
        print('   ‚ùå 6-PHASE TRAINING: FAILED')
        return False

def debug_full_pipeline():
    """Debug the complete 3 batch 3 phase pipeline using current architecture"""
    print('\nüöÄ DEBUG 4: Full Pipeline (3 Batch 3 Phase)')
    print('='*70)
    
    try:
        # Initialize components using current architecture
        env = TetrisEnv()
        exploration_manager = PieceByPieceExplorationManager(env)
        components = Enhanced6PhaseComponents(device='cpu')
        components.set_optimizers(state_lr=0.001, q_lr=0.001)
        
        # Configure for quick test
        exploration_manager.max_pieces = 2  # Quick test with 2 pieces
        exploration_manager.boards_to_keep = 3
        num_batches = 1  # CHANGED: 1 batch per stage as requested
        num_phases = 3   # CHANGED: 3 stages as requested
        
        print(f'‚öôÔ∏è Configuration:')
        print(f'   ‚Ä¢ Batches per phase: {num_batches}')
        print(f'   ‚Ä¢ Training phases: {num_phases}')
        print(f'   ‚Ä¢ Max pieces per exploration: {exploration_manager.max_pieces}')
        print(f'   ‚Ä¢ Boards to keep: {exploration_manager.boards_to_keep}')
        
        # Run the training pipeline manually
        print(f'\nüèÉ‚Äç‚ôÇÔ∏è Running full training pipeline...')
        
        total_state_loss = 0
        total_q_loss = 0
        total_exploration_data = 0
        
        # 3 phases √ó 3 batches = 9 training cycles
        for phase in range(num_phases):
            print(f'\nüìà PHASE {phase + 1}/{num_phases}')
            
            for batch in range(num_batches):
                print(f'   Batch {batch + 1}/{num_batches}...')
                
                # 1. Collect exploration data
                exploration_data = exploration_manager.collect_piece_by_piece_exploration_data('iterative')
                total_exploration_data += len(exploration_data)
                
                if len(exploration_data) < 5:
                    print(f'   ‚ö†Ô∏è Insufficient exploration data: {len(exploration_data)}')
                    continue
                
                # 2. Train state model
                state_results = components.train_enhanced_state_model(exploration_data)
                total_state_loss += state_results['loss']
                
                # 3. Train Q-learning
                q_results = components.train_simplified_q_learning(exploration_data)
                total_q_loss += q_results['q_loss']
                
                print(f'     ‚Ä¢ Explored states: {len(exploration_data)}')
                print(f'     ‚Ä¢ State loss: {state_results["loss"]:.3f}')
                print(f'     ‚Ä¢ Q loss: {q_results["q_loss"]:.3f}')
        
        # Calculate final statistics
        avg_state_loss = total_state_loss / (num_phases * num_batches)
        avg_q_loss = total_q_loss / (num_phases * num_batches)
        
        final_stats = {
            'phase': num_phases,
            'total_exploration_data': total_exploration_data,
            'avg_state_loss': avg_state_loss,
            'avg_q_loss': avg_q_loss,
            'exploration_reward': 'computed_via_iterative',
            'actor_loss': avg_state_loss  # Use state loss as proxy
        }
        
        print(f'\nüìä PIPELINE RESULTS:')
        print(f'   ‚Ä¢ Training completed successfully: True')
        print(f'   ‚Ä¢ Total exploration data: {final_stats["total_exploration_data"]}')
        print(f'   ‚Ä¢ Average state loss: {final_stats["avg_state_loss"]:.3f}')
        print(f'   ‚Ä¢ Average Q loss: {final_stats["avg_q_loss"]:.3f}')
        print(f'   ‚Ä¢ Training phases completed: {final_stats["phase"]}')
        print('   ‚úÖ FULL PIPELINE: WORKING')
        return True, final_stats
            
    except Exception as e:
        print(f'   ‚ùå FULL PIPELINE: FAILED with error: {e}')
        import traceback
        traceback.print_exc()
        return False, None

def main():
    """Run comprehensive debug session"""
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    
    print('üîß COMPREHENSIVE SYSTEM DEBUG')
    print('='*80)
    print(f'Timestamp: {timestamp}')
    print(f'PyTorch version: {torch.__version__}')
    print(f'Device: cpu')
    
    # Run all debug tests
    debug_results = {}
    
    # 1. Iterative exploration
    iter_works, iter_data = debug_iterative_exploration()
    debug_results['iterative_exploration'] = iter_works
    
    # 2. Goal vector system
    goal_works = debug_goal_vector_system()
    debug_results['goal_vector_system'] = goal_works
    
    # 3. 6-phase training
    training_works = debug_6phase_training()
    debug_results['6phase_training'] = training_works
    
    # 4. Full pipeline (if basics work)
    if iter_works and goal_works and training_works:
        pipeline_works, pipeline_stats = debug_full_pipeline()
        debug_results['full_pipeline'] = pipeline_works
    else:
        print('\n‚è≠Ô∏è Skipping full pipeline test - prerequisites failed')
        debug_results['full_pipeline'] = False
    
    # Summary
    print('\nüéâ DEBUG SESSION SUMMARY')
    print('='*80)
    working_count = sum(debug_results.values())
    total_count = len(debug_results)
    
    for test_name, result in debug_results.items():
        status = "‚úÖ PASS" if result else "‚ùå FAIL"
        print(f'   ‚Ä¢ {test_name.replace("_", " ").title()}: {status}')
    
    print(f'\nüìä Overall: {working_count}/{total_count} systems working')
    
    if working_count == total_count:
        print('üöÄ ALL SYSTEMS OPERATIONAL - Ready for production training!')
    else:
        print('‚ö†Ô∏è Issues detected - Review failed systems before production use')
    
    # Save debug results
    debug_file = f'debug_results_{timestamp}.txt'
    with open(debug_file, 'w') as f:
        f.write(f'Debug Session Results - {timestamp}\n')
        f.write('='*50 + '\n\n')
        for test_name, result in debug_results.items():
            f.write(f'{test_name}: {"PASS" if result else "FAIL"}\n')
        f.write(f'\nOverall: {working_count}/{total_count} systems working\n')
    
    print(f'\nüíæ Debug results saved to: {debug_file}')
    
    return debug_results

if __name__ == '__main__':
    try:
        results = main()
    except Exception as e:
        print(f'\nüí• Debug session crashed: {e}')
        import traceback
        traceback.print_exc() 